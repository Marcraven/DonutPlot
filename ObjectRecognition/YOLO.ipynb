{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "# Functions to generate simple plots\n",
        "\n",
        "\n",
        "def create_scatterplot_with_coco_bounding_boxes(num_points=10, size=(256, 256), dot_size=3):\n",
        "    # Initialize an image with ones (white)\n",
        "    image = np.ones((size[0], size[1], 3), dtype=np.uint8) * 255\n",
        "\n",
        "    # Generate random points\n",
        "    points = np.random.randint(0, size[0], size=(num_points, 2))\n",
        "\n",
        "    # Bounding boxes list\n",
        "    bounding_boxes = []\n",
        "\n",
        "    # Drawing points on the image and creating bounding boxes\n",
        "    for point in points:\n",
        "        x, y = point\n",
        "        # Draw a bigger dot\n",
        "        image[y-dot_size:y+dot_size+1, x-dot_size:x+dot_size+1] = [0, 0, 0]  # Black point\n",
        "\n",
        "        # Bounding box for each point (in COCO format)\n",
        "        bb_width = bb_height = 2 * dot_size\n",
        "        x_center = x / size[0]\n",
        "        y_center = y / size[1]\n",
        "        width_normalized = bb_width / size[0]\n",
        "        height_normalized = bb_height / size[1]\n",
        "\n",
        "        bounding_box = (x_center, y_center, width_normalized, height_normalized)\n",
        "        bounding_boxes.append(bounding_box)\n",
        "\n",
        "    return image, bounding_boxes\n",
        "\n",
        "def save_coco_bounding_boxes_to_txt(bounding_boxes, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        for box in bounding_boxes:\n",
        "            x_center, y_center, width, height = box\n",
        "            x1 = x_center - width / 2\n",
        "            y1 = y_center - height / 2\n",
        "            x2 = x_center + width / 2\n",
        "            y2 = y_center + height / 2\n",
        "            file.write(f'0 {x1} {y1} {width} {height}\\n')\n",
        "\n",
        "def read_coco_bounding_boxes_from_txt(filename):\n",
        "    bounding_boxes = []\n",
        "    with open(filename, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            data = line.strip().split()\n",
        "            category_id = int(data[0])\n",
        "            x1, y1, width, height = map(float, data[1:])\n",
        "            x_center = x1 + width / 2\n",
        "            y_center = y1 + height / 2\n",
        "            bounding_box = (x_center, y_center, width, height)\n",
        "            bounding_boxes.append(bounding_box)\n",
        "    return bounding_boxes\n",
        "\n",
        "def visualize_coco_bounding_boxes(image, bounding_boxes, size=(256, 256)):\n",
        "    # Iterate over the bounding boxes and draw them\n",
        "    for box in bounding_boxes:\n",
        "        x_center, y_center, width, height = box\n",
        "        x_center *= size[0]\n",
        "        y_center *= size[1]\n",
        "        width *= size[0]\n",
        "        height *= size[1]\n",
        "\n",
        "        # Convert COCO format to top-left and bottom-right coordinates\n",
        "        x1 = int(x_center - width / 2)\n",
        "        y1 = int(y_center - height / 2)\n",
        "        x2 = int(x_center + width / 2)\n",
        "        y2 = int(y_center + height / 2)\n",
        "\n",
        "        # Draw rectangle (bounding box)\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 1)  # Red box\n",
        "\n",
        "    return image\n",
        "\n",
        "# Generate image and bounding boxes\n",
        "scatter_image, coco_boxes = create_scatterplot_with_coco_bounding_boxes()\n",
        "\n",
        "# # Save the generated image\n",
        "# image_path = 'scatterplot.png'\n",
        "# cv2.imwrite(image_path, scatter_image)\n",
        "\n",
        "# # Save the bounding boxes to a text file in COCO format\n",
        "# bounding_boxes_path = 'bounding_boxes.txt'\n",
        "# save_coco_bounding_boxes_to_txt(coco_boxes, bounding_boxes_path)\n",
        "\n",
        "# # Read the image and bounding box information from the files\n",
        "# loaded_image = cv2.imread(image_path)\n",
        "# loaded_coco_boxes = read_coco_bounding_boxes_from_txt(bounding_boxes_path)\n",
        "\n",
        "# # Visualize the bounding boxes on the loaded image\n",
        "# visualized_image = visualize_coco_bounding_boxes(loaded_image.copy(), loaded_coco_boxes)\n",
        "\n",
        "# # Display or save the visualized image\n",
        "# visualized_image_path = 'visualized_coco_scatterplot.png'\n",
        "# cv2.imwrite(visualized_image_path, visualized_image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create folders to fill in with data\n",
        "import os\n",
        "base_dir='yolo'\n",
        "dirs =  ['train', 'test', 'val']\n",
        "for subset in dirs:\n",
        "    img_dir = os.path.join(base_dir, 'images', subset)\n",
        "    label_dir = os.path.join(base_dir, 'labels', subset)\n",
        "    os.makedirs(img_dir, exist_ok=True)\n",
        "    os.makedirs(label_dir, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.pyenv/versions/3.10.6/envs/DonutPlot/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "# Train the model with Ultralytics - wrap around of Pytorch Yolo model\n",
        "# yolov8s-p2 - version for detecting smaller objects on images\n",
        "# https://github.com/ultralytics/ultralytics/issues/981  - you can find out more here\n",
        "def train_model():\n",
        "    # Load the pre-trained model\n",
        "    model = YOLO(\"yolov8s-p2.yaml\").load(\"yolov8s.pt\")\n",
        "\n",
        "    # Train the model\n",
        "    model.train(\n",
        "        data=\"dataset.yaml\", epochs=200,  imgsz=320, save=True, format='onnx'\n",
        "    )  # Set imgsz to 320 for training on 320xsomething images\n",
        "\n",
        "    # Export the model to ONNX format\n",
        "    path = model.export()\n",
        "    print(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 17             [-1, 2]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 19                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 20            [-1, 15]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 22                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 23            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 24                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 25                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 26             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 27                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 28    [18, 21, 24, 27]  1   1666896  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256, 512]]     \n",
            "YOLOv8s-p2 summary: 277 layers, 10884336 parameters, 10884320 gradients, 39.7 GFLOPs\n",
            "\n",
            "Transferred 219/437 items from pretrained weights\n",
            "Ultralytics YOLOv8.0.220 🚀 Python-3.10.6 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce GTX 1060, 6144MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s-p2.yaml, data=dataset.yaml, epochs=1, patience=50, batch=16, imgsz=320, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train22, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/root/.pyenv/runs/detect/train22\n",
            "Overriding model.yaml nc=80 with nc=21\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 17             [-1, 2]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 19                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 20            [-1, 15]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 22                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 23            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 24                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 25                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 26             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 27                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 28    [18, 21, 24, 27]  1   1424996  ultralytics.nn.modules.head.Detect           [21, [64, 128, 256, 512]]     \n",
            "YOLOv8s-p2 summary: 277 layers, 10642436 parameters, 10642420 gradients, 37.0 GFLOPs\n",
            "\n",
            "Transferred 389/437 items from pretrained weights\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/marcelo-nora/general/e15deb5f5cb64bbc95b48f25c6dc821a\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /root/.pyenv/runs/detect/train22', view at http://localhost:6006/\n",
            "Freezing layer 'model.28.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6.23M/6.23M [00:28<00:00, 229kB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ NMS time limit 0.550s exceeded\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /root/code/marcraven/DonutPlot/ObjectRecognition/yolo/dataset/train.cache... 100 images, 0 backgrounds, 0 corrupt: 100%|██████████| 100/100 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /root/code/marcraven/DonutPlot/ObjectRecognition/yolo/dataset/validation.cache... 12 images, 0 backgrounds, 0 corrupt: 100%|██████████| 12/12 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to /root/.pyenv/runs/detect/train22/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0004, momentum=0.9) with parameter groups 70 weight(decay=0.0), 79 weight(decay=0.0005), 78 bias(decay=0.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023/11/29 14:44:40 INFO mlflow.tracking.fluent: Experiment with name 'donut_experiment' does not exist. Creating a new experiment.\n",
            "2023/11/29 14:44:40 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of transformers. If you encounter errors during autologging, try upgrading / downgrading transformers to a supported version, or try upgrading MLflow.\n",
            "2023/11/29 14:44:43 INFO mlflow.tracking.fluent: Autologging successfully enabled for transformers.\n",
            "2023/11/29 14:44:44 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/marcelo-nora/general/e15deb5f5cb64bbc95b48f25c6dc821a\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Created from                 : yolov8\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_batch_logging_interval  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     log_confusion_matrix_on_eval : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     log_image_predictions        : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     max_image_predictions        : 100\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     agnostic_nms    : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     amp             : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     augment         : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch           : 16\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     box             : 7.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cache           : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cfg             : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     classes         : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     close_mosaic    : 10\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cls             : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conf            : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     copy_paste      : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cos_lr          : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data            : dataset.yaml\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     degrees         : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     deterministic   : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     device          : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dfl             : 1.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dnn             : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout         : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dynamic         : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs          : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     exist_ok        : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fliplr          : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flipud          : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     format          : torchscript\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fraction        : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     freeze          : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     half            : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_h           : 0.015\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_s           : 0.7\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_v           : 0.4\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     imgsz           : 320\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     int8            : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     iou             : 0.7\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     keras           : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kobj            : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     label_smoothing : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     line_width      : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr0             : 0.01\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lrf             : 0.01\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mask_ratio      : 4\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     max_det         : 300\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mixup           : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mode            : train\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model           : yolov8s-p2.yaml\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     momentum        : 0.937\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mosaic          : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name            : train22\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     nbs             : 64\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     nms             : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     opset           : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimize        : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer       : auto\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     overlap_mask    : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     patience        : 50\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     perspective     : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     plots           : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pose            : 12.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pretrained      : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     profile         : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     project         : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     rect            : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     resume          : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     retina_masks    : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save            : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_conf       : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_crop       : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_dir        : /root/.pyenv/runs/detect/train22\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_frames     : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_hybrid     : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_json       : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_period     : -1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_txt        : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     scale           : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     seed            : 0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     shear           : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show            : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show_boxes      : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show_conf       : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show_labels     : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     simplify        : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     single_cls      : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source          : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     split           : val\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stream_buffer   : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     task            : detect\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     tracker         : botsort.yaml\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     translate       : 0.1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val             : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     verbose         : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     vid_stride      : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     visualize       : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_bias_lr  : 0.1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_epochs   : 3.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_momentum : 0.8\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     weight_decay    : 0.0005\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     workers         : 8\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     workspace       : 4\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (13.32 KB)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages              : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/marcelo-nora/donut-experiment/f241f639f5894ce689896b4fbfa4a597\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(a42e580c48524b038ca69d8207693801) to https://mlflow.lewagon.ai\n",
            "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
            "Image sizes 320 train, 320 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1m/root/.pyenv/runs/detect/train22\u001b[0m\n",
            "Starting training for 1 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        1/1      2.75G      5.439      5.843        4.2         78        320: 100%|██████████| 7/7 [00:10<00:00,  1.46s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         12        277          0          0          0          0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1 epochs completed in 0.009 hours.\n",
            "Optimizer stripped from /root/.pyenv/runs/detect/train22/weights/last.pt, 21.6MB\n",
            "Optimizer stripped from /root/.pyenv/runs/detect/train22/weights/best.pt, 21.6MB\n",
            "\n",
            "Validating /root/.pyenv/runs/detect/train22/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.220 🚀 Python-3.10.6 torch-2.1.1+cu121 CUDA:0 (NVIDIA GeForce GTX 1060, 6144MiB)\n",
            "YOLOv8s-p2 summary (fused): 207 layers, 10631908 parameters, 0 gradients, 36.7 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         12        277          0          0          0          0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Speed: 1.3ms preprocess, 28.6ms inference, 0.0ms loss, 5.0ms postprocess per image\n",
            "Results saved to \u001b[1m/root/.pyenv/runs/detect/train22\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m We failed to read file /root/.pyenv/runs/detect/train22/F1_curve.png for uploading.\n",
            "Please double-check the file path and permissions\n",
            "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m We failed to read file /root/.pyenv/runs/detect/train22/P_curve.png for uploading.\n",
            "Please double-check the file path and permissions\n",
            "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m We failed to read file /root/.pyenv/runs/detect/train22/R_curve.png for uploading.\n",
            "Please double-check the file path and permissions\n",
            "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m We failed to read file /root/.pyenv/runs/detect/train22/PR_curve.png for uploading.\n",
            "Please double-check the file path and permissions\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/marcelo-nora/donut-experiment/f241f639f5894ce689896b4fbfa4a597\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss                        : 249.51290893554688\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr/pg0                      : 2.4e-05\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr/pg1                      : 2.4e-05\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr/pg2                      : 2.4e-05\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/mAP50(B)            : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/mAP50-95(B)         : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/mAP50-95B           : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/mAP50B              : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/precision(B)        : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/precisionB          : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/recall(B)           : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/recallB             : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/GFLOPs                : 37.034\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/parameters            : 10642436\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/speed_PyTorch(ms) [2] : (28.618, 31.213)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/box_loss [8]          : (5.40868, 5.63984)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/cls_loss [8]          : (5.78858, 5.84303)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/dfl_loss [8]          : (4.18461, 4.20038)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/box_loss                : 5.8255\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/cls_loss                : 5.81318\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/dfl_loss                : 4.16363\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : train22\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     agnostic_nms    : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     amp             : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     augment         : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch           : 16\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     box             : 7.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cache           : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cfg             : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     classes         : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     close_mosaic    : 10\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cls             : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conf            : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     copy_paste      : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cos_lr          : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data            : dataset.yaml\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     degrees         : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     deterministic   : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     device          : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dfl             : 1.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dnn             : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout         : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dynamic         : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs          : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     exist_ok        : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fliplr          : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flipud          : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     format          : torchscript\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fraction        : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     freeze          : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     half            : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_h           : 0.015\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_s           : 0.7\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_v           : 0.4\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     imgsz           : 320\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     int8            : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     iou             : 0.7\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     keras           : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kobj            : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     label_smoothing : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     line_width      : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr0             : 0.01\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lrf             : 0.01\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mask_ratio      : 4\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     max_det         : 300\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mixup           : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mode            : train\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model           : yolov8s-p2.yaml\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     momentum        : 0.937\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mosaic          : 1.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name            : train22\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     nbs             : 64\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     nms             : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     opset           : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimize        : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer       : auto\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     overlap_mask    : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     patience        : 50\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     perspective     : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     plots           : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pose            : 12.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pretrained      : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     profile         : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     project         : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     rect            : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     resume          : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     retina_masks    : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save            : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_conf       : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_crop       : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_dir        : /root/.pyenv/runs/detect/train22\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_frames     : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_hybrid     : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_json       : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_period     : -1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_txt        : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     scale           : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     seed            : 0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     shear           : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show            : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show_boxes      : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show_conf       : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show_labels     : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     simplify        : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     single_cls      : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source          : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     split           : val\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stream_buffer   : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     task            : detect\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     tracker         : botsort.yaml\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     translate       : 0.1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val             : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     verbose         : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     vid_stride      : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     visualize       : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_bias_lr  : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_epochs   : 3.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_momentum : 0.8\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     weight_decay    : 0.0005\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     workers         : 8\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     workspace       : 4\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     confusion-matrix         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (26.79 KB)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     images                   : 6\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph              : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model-element            : 1 (20.56 MB)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages              : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for assets to finish uploading (timeout is 10800 seconds)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m All assets have been sent, waiting for delivery confirmation\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to https://mlflow.lewagon.ai\n",
            "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
            "Ultralytics YOLOv8.0.220 🚀 Python-3.10.6 torch-2.1.1+cu121 CPU (Intel Core(TM) i7-7700HQ 2.80GHz)\n",
            "YOLOv8s-p2 summary (fused): 207 layers, 10631908 parameters, 0 gradients, 36.7 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/root/.pyenv/runs/detect/train22/weights/best.pt' with input shape (1, 3, 320, 320) BCHW and output shape(s) (1, 25, 8500) (20.6 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.1.1+cu121...\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success ✅ 5.2s, saved as '/root/.pyenv/runs/detect/train22/weights/best.torchscript' (41.1 MB)\n",
            "\n",
            "Export complete (9.2s)\n",
            "Results saved to \u001b[1m/root/.pyenv/runs/detect/train22/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=/root/.pyenv/runs/detect/train22/weights/best.torchscript imgsz=320  \n",
            "Validate:        yolo val task=detect model=/root/.pyenv/runs/detect/train22/weights/best.torchscript imgsz=320 data=dataset.yaml  \n",
            "Visualize:       https://netron.app\n",
            "/root/.pyenv/runs/detect/train22/weights/best.torchscript\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import comet_ml\n",
        "\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    comet_ml.init()\n",
        "    # Load the pre-trained model\n",
        "    model = YOLO(\"yolov8s-p2.yaml\").load(\"ObjectRecognition/yolov8s.pt\")\n",
        "\n",
        "    # Train the model\n",
        "    model.train(\n",
        "        data= \"dataset.yaml\",\n",
        "        # data=\"dataset.yaml\",\n",
        "        epochs=1,\n",
        "        imgsz=320,\n",
        "        save=True,  # device=\"gpu\"\n",
        "    )  # Set imgsz to 320 for training on 320xsomething images\n",
        "\n",
        "    # Export the model to ONNX format\n",
        "\n",
        "    path = model.export()\n",
        "    print(path)\n",
        "\n",
        "\n",
        "train_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I WOULD HIGHLY RECCOMEND TRAINING USING COMET (its like MLFLOW)\n",
        "# https://docs.ultralytics.com/modes/train/#logging - HOW TO\n",
        "# you will have all the weights and training progress save there inside of experiments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'best-30.pt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/marcraven/code/Marcraven/DonutPlot/ObjectRecognition/Train simple YOLO model to detect scatter points.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/marcraven/code/Marcraven/DonutPlot/ObjectRecognition/Train%20simple%20YOLO%20model%20to%20detect%20scatter%20points.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Example of predictions\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/marcraven/code/Marcraven/DonutPlot/ObjectRecognition/Train%20simple%20YOLO%20model%20to%20detect%20scatter%20points.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39multralytics\u001b[39;00m \u001b[39mimport\u001b[39;00m YOLO\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/marcraven/code/Marcraven/DonutPlot/ObjectRecognition/Train%20simple%20YOLO%20model%20to%20detect%20scatter%20points.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m YOLO(\u001b[39m'\u001b[39;49m\u001b[39mbest-30.pt\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m# Load this from\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/marcraven/code/Marcraven/DonutPlot/ObjectRecognition/Train%20simple%20YOLO%20model%20to%20detect%20scatter%20points.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m results \u001b[39m=\u001b[39m model(\u001b[39m'\u001b[39m\u001b[39mObjectRecognition/yolo/dataset/test/0000.jpg\u001b[39m\u001b[39m'\u001b[39m, imgsz\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, save\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, conf\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/DonutPlot/lib/python3.10/site-packages/ultralytics/engine/model.py:94\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model, task)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(model, task)\n\u001b[1;32m     93\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load(model, task)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/DonutPlot/lib/python3.10/site-packages/ultralytics/engine/model.py:146\u001b[0m, in \u001b[0;36mModel._load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    144\u001b[0m suffix \u001b[39m=\u001b[39m Path(weights)\u001b[39m.\u001b[39msuffix\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m suffix \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt \u001b[39m=\u001b[39m attempt_load_one_weight(weights)\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mtask\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moverrides \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset_ckpt_args(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39margs)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/DonutPlot/lib/python3.10/site-packages/ultralytics/nn/tasks.py:628\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mattempt_load_one_weight\u001b[39m(weight, device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fuse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    627\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Loads a single model weights.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m     ckpt, weight \u001b[39m=\u001b[39m torch_safe_load(weight)  \u001b[39m# load ckpt\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     args \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mDEFAULT_CFG_DICT, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(ckpt\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtrain_args\u001b[39m\u001b[39m'\u001b[39m, {}))}  \u001b[39m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     model \u001b[39m=\u001b[39m (ckpt\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mema\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m ckpt[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat()  \u001b[39m# FP32 model\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/DonutPlot/lib/python3.10/site-packages/ultralytics/nn/tasks.py:567\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m     \u001b[39mwith\u001b[39;00m temporary_modules({\n\u001b[1;32m    564\u001b[0m             \u001b[39m'\u001b[39m\u001b[39multralytics.yolo.utils\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39multralytics.utils\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    565\u001b[0m             \u001b[39m'\u001b[39m\u001b[39multralytics.yolo.v8\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39multralytics.models.yolo\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    566\u001b[0m             \u001b[39m'\u001b[39m\u001b[39multralytics.yolo.data\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39multralytics.data\u001b[39m\u001b[39m'\u001b[39m}):  \u001b[39m# for legacy 8.0 Classify and Pose models\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(file, map_location\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m), file  \u001b[39m# load\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# e.name is missing module name\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m'\u001b[39m:\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/DonutPlot/lib/python3.10/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/DonutPlot/lib/python3.10/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    436\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/DonutPlot/lib/python3.10/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best-30.pt'"
          ]
        }
      ],
      "source": [
        "# Example of predictions\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('best-30.pt') # Load this from\n",
        "results = model('ObjectRecognition/yolo/dataset/test/0000.jpg', imgsz=256, save=True, conf=0.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lewagon",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
